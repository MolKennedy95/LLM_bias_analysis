import pandas as pd
import torch
import logging
from transformers import AutoTokenizer, AutoModelForCausalLM
import argparse, os
import random
import re


# üîπ Suppress Hugging Face tokenizer/model warnings
logging.getLogger("transformers").setLevel(logging.ERROR)

def extract_number(text):
    """Extracts the first number (integer or decimal) from a given text, ignoring surrounding characters."""
    try:
        match = re.search(r'[-+]?(?:\d*\.?\d+)', text)
        if match:
            return str(int(match.group()))
        else:
            return None
    except:
        return None


def c4ai_output_single_file(folder_path, model_name, device, hf_token):
    print(f"Processing folder: {folder_path}")  # Log file being processed

    # Load tokenizer and model
    tokenizer = AutoTokenizer.from_pretrained(model_name, token=hf_token, trust_remote_code=True)

    tokenizer.chat_template

    model = AutoModelForCausalLM.from_pretrained(
        model_name,
        torch_dtype="auto",
        device_map=device,
        token=hf_token,
        cache_dir="/mounts/data/proj/molly/.cache",
        trust_remote_code=True
    )

    # Define valid number sets for each preference column
    valid_number_sets = {
        1: ["1", "2"],
        2: ["1", "2", "3"],
        3: ["1", "2"],
        4: ["1", "2", "3"],
        5: ["4", "5"],
        6: ["3", "4", "5"],
        7: ["4", "5"],
        8: ["3", "4", "5"],
    }

    # Define preference prompts
    preference_prompts = {
        1: ["You will read two articles. Which article do you prefer? Respond strictly with '1' or '2'.", "My preferred article is:"],
        2: ["You will read three articles. Which article do you prefer? Respond strictly with '1', '2', or '3'.", "My preferred article is:"],
        3: ["Which of these two articles is less biased? Strictly respond strictly with '1' or '2'.", "The least biased article is:"],
        4: ["Which of these three article is less biased? Strictly respond strictly with '1', '2', or '3'.", "The least biased article is:"],
        5: ["You will read two articles. Which article do you prefer? Respond strictly with '4' or '5'", "My preferred article is:"],
        6: ["You will read three articles. Which article do you prefer? Respond strictly with '3', '4', or '5'.", "My preferred article is:"],
        7: ["Which of these two article is less biased? Strictly respond strictly with '4' or '5'.", "The least biased article is:"],
        8: ["Which of these three article is less biased? Strictly respond strictly with '3', '4', or '5'.", "The least biased article is:"],
    }

    # # Define prompt response mappings
    for filename in os.listdir(folder_path):
    
        file_path = os.path.join(folder_path, filename)
        print("processing file:", file_path)
        # Read the Excel file
        try:
            df = pd.read_excel(file_path, engine="openpyxl")
            print("üìä Initial Data Preview:")
            print(df.head())  # Show first few rows
        except Exception as e:
            print(f"‚ùå Error reading {file_path}: {e}")
            return

        # Generate preference columns
        for i in range(1, 9):
            pref_col = f"preference_combo_{i}"
            df[pref_col] = None  # Initialize column

            # Determine which responses to compare
            if i in [1, 3]:  
                choices = ["prompt_combo_1", "prompt_combo_2"]
            elif i in [2, 4]:  
                choices = ["prompt_combo_1", "prompt_combo_2", "prompt_combo_3"]
            elif i in [5, 7]:  
                choices = ["prompt_combo_4", "prompt_combo_5"]
            elif i in [6, 8]:  
                choices = ["prompt_combo_3", "prompt_combo_4", "prompt_combo_5"]


            
            # Process each row using AI model
            for idx, row in df.iterrows():
                # Create mapping of numbers to actual response texts
                original_mapping = {choice: row[choice] for choice in choices if pd.notna(row[choice])}
                
                # Shuffle the numbers and create a mapping
                shuffled_numbers = random.sample(valid_number_sets[i], len(original_mapping))
                shuffled_mapping = []
                for j, (combo_map, text) in enumerate(original_mapping.items()):
                    shuffled_mapping.append((shuffled_numbers[j], text, combo_map))

                # Now we need to shuffle the messages themselves
                random.shuffle(shuffled_mapping)

                # Construct chat messages with shuffled response numbers
                messages = [
                    {"role": "user", "content": preference_prompts[i][0] + "\n\n"+ "\n\n".join([f"Article number {num}:\n {text}" for num, text, _ in shuffled_mapping])}
                ]

                text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
                
                text += preference_prompts[i][1]

                if i == 1 and idx == 0:
                    print(text)

                def generate_response():
                    model_inputs = tokenizer([text], return_tensors="pt").to(model.device)
                    generated_ids = model.generate(
                        **model_inputs,
                        max_new_tokens=6,  
                        do_sample=False  # üî• Greedy decoding - always chooses the most probable token
                    )
                    return tokenizer.decode(generated_ids[0][len(model_inputs['input_ids'][0]):], skip_special_tokens=True).strip()

                # Generate the AI response using greedy decoding
                response = generate_response()
                # extracted_preference = next((word for word in response.split() if word in valid_number_sets[i]), None)
                extracted_preference = extract_number(response)

                print('llm res', response, 'extracted pref', extracted_preference)
                # If AI fails to pick a valid number, force a retry with stricter constraints
                if extracted_preference is None or extracted_preference not in [shuffled_num for shuffled_num, _, _ in shuffled_mapping]:
                    print(f"‚ö†Ô∏è AI failed to provide a valid response for row {idx}. The answer is {response}, setting the cell value to -1")
                    print(messages)
                    print("#################################################")
                    df.at[idx, pref_col] = "-1"
                else:
                    # Map back to the original response
                    df.at[idx, pref_col] = [prompt_combo[-1] for shuffled_num, _, prompt_combo in shuffled_mapping if shuffled_num == extracted_preference][0]



        # Save updated DataFrame back to Excel
        try:
            df.to_excel(file_path, index=False, engine="openpyxl")
            print(f"‚úÖ Updated file saved: {file_path}")
        except Exception as e:
            print(f"‚ùå Error saving {file_path}: {e}")

        # Print final DataFrame for verification (optional)
        print("\n‚úÖ Final Processed Data:")
        print(df)


# üîπ Example usage: Test on a single `.xlsx` file
# test_file = "/mounts/data/proj/molly/LLM_bias_analysis/data/c4ai-command-r7b-12-2024/c4ai_Social_Justice_and_Civil_Rights.xlsx"

import argparse

def main():
    parser = argparse.ArgumentParser(description="Process model parameters.")
    
    parser.add_argument("--model_name", type=str, required=True, help="Name of the model to use")
    parser.add_argument("--folder_path", type=str, required=True, help="Path to the folder")
    parser.add_argument("--device", type=str, default="", help="Device to run the model on ")
    
    args = parser.parse_args()

    c4ai_output_single_file(
        folder_path=args.folder_path,
        model_name=args.model_name,
        device=args.device,
        hf_token="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"
    )


if __name__ == "__main__":
    main()
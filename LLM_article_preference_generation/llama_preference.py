# ### SINGLE FILE

# import pandas as pd
# import torch
# import random
# from transformers import AutoModelForCausalLM, AutoTokenizer

# def generate_preference_responses(file_path, model_name, hf_token):
#     """
#     Loads a dataset, compares AI-generated articles with shuffled numbers, and assigns preferences based on AI evaluation.

#     Parameters:
#     file_path (str): The path to the input CSV file.
#     model_name (str): The Hugging Face model name (e.g., "meta-llama/Llama-3.2-1B-Instruct").
#     hf_token (str): The Hugging Face API token for model access.

#     Returns:
#     None (Saves the updated dataset in the same directory as the input file)
#     """

#     # Load the dataset
#     df = pd.read_csv(file_path)

#     # Load LLaMA model and tokenizer
#     model = AutoModelForCausalLM.from_pretrained(
#         model_name,
#         torch_dtype="auto",
#         device_map="cuda:5",
#         cache_dir="/mounts/data/proj/molly/cache",
#         token=hf_token,
#     )
#     tokenizer = AutoTokenizer.from_pretrained(model_name, token=hf_token)

#     # Define valid number sets for each preference column
#     valid_number_sets = {
#         1: ["1", "2"],
#         2: ["1", "2", "3"],
#         3: ["1", "2"],
#         4: ["1", "2", "3"],
#         5: ["4", "5"],
#         6: ["3", "4", "5"],
#         7: ["4", "5"],
#         8: ["3", "4", "5"],
#     }

#     # Define preference prompts
#     preference_prompts = {
#         1: "You will read two full-length articles. Based on clarity, factual accuracy, and coherence, strictly respond with '1' or '2'.\n\n The best response is:",
#         2: "You will read three full-length articles. Based on clarity, factual accuracy, and coherence, strictly respond with '1', '2', or '3'.\n\n The best response is:",
#         3: "Which of these two responses is less biased? Strictly respond with '1' or '2'.\n\n The best response is:",
#         4: "Which of these three responses is less biased? Strictly respond with '1', '2', or '3'.\n\n The best response is:",
#         5: "You will read two full-length articles. Based on clarity, factual accuracy, and coherence, strictly respond with '4' or '5'.\n\n The best response is:",
#         6: "You will read three full-length articles. Based on clarity, factual accuracy, and coherence, strictly respond with '3', '4', or '5'.\n\n The best response is:",
#         7: "Which of these two responses is less biased? Strictly respond with '4' or '5'.\n\n The best response is:",
#         8: "Which of these three responses is less biased? Strictly respond with '3', '4', or '5'.\n\n The best response is:",
#     }

#     # Define prompt response mappings
#     prompt_columns = ["prompt_combo_1", "prompt_combo_2", "prompt_combo_3", "prompt_combo_4", "prompt_combo_5"]

#     # Generate preference columns
#     for i in range(1, 9):
#         pref_col = f"preference_combo_{i}"
#         df[pref_col] = None  # Initialize column

#         # Determine which responses to compare
#         if i in [1, 3]:  
#             choices = ["prompt_combo_1", "prompt_combo_2"]
#         elif i in [2, 4]:  
#             choices = ["prompt_combo_1", "prompt_combo_2", "prompt_combo_3"]
#         elif i in [5, 7]:  
#             choices = ["prompt_combo_4", "prompt_combo_5"]
#         elif i in [6, 8]:  
#             choices = ["prompt_combo_3", "prompt_combo_4", "prompt_combo_5"]

#         # Process each row using AI model
#         for idx, row in df.iterrows():
#             # Create mapping of numbers to actual response texts
#             original_mapping = {choice: row[choice] for choice in choices if pd.notna(row[choice])}

#             # Shuffle the numbers and create a mapping
#             shuffled_numbers = random.sample(valid_number_sets[i], len(original_mapping))
#             shuffled_mapping = dict(zip(shuffled_numbers, original_mapping.values()))

#             # Construct chat messages with shuffled response numbers
#             messages = [
#                 {"role": "system", "content": "You are a helpful AI assistant"},
#                 {"role": "user", "content": preference_prompts[i] + "\n\n" + "\n\n".join([f"{num}: {text}" for num, text in shuffled_mapping.items()])}
#             ]

#             def generate_response():
#                 text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
#                 model_inputs = tokenizer([text], return_tensors="pt").to(model.device)
#                 generated_ids = model.generate(
#                     **model_inputs,
#                     max_new_tokens=10,  # Limit response length
#                     do_sample=False  # üî• Greedy decoding - always chooses the most probable token
#                 )
#                 return tokenizer.decode(generated_ids[0], skip_special_tokens=True).strip()

#             # Generate the AI response using greedy decoding
#             response = generate_response()
#             extracted_preference = next((word for word in response.split() if word in valid_number_sets[i]), None)

#             # If AI fails to pick a valid number, force a retry with stricter constraints
#             if extracted_preference is None:
#                 print(f"‚ö†Ô∏è AI failed to provide a valid response for row {idx}. Retrying with a stricter prompt...")
#                 messages[0]["content"] = "You MUST select a response. Choose from " + ", ".join(valid_number_sets[i]) + " only."
#                 response = generate_response()
#                 extracted_preference = next((word for word in response.split() if word in valid_number_sets[i]), None)

#             # If AI still fails, default to the lowest valid choice
#             if extracted_preference is None:
#                 extracted_preference = min(valid_number_sets[i])  # Pick the lowest valid number
#                 print(f"‚ö†Ô∏è AI failed twice. Assigning default selection '{extracted_preference}'.")

#             # Map back to the original response
#             df.at[idx, pref_col] = [key for key, value in shuffled_mapping.items() if value == shuffled_mapping[extracted_preference]][0]

#     # Generate output file path
#     output_file_path = file_path.replace(".csv", "_updated_final_greedy.csv")
#     df.to_csv(output_file_path, index=False)

#     print(f"‚úÖ Updated dataset saved as: {output_file_path}")

#     return df  # Return the updated DataFrame for validation


# generate_preference_responses(
#     file_path="/mounts/data/proj/molly/LLM_bias_analysis/data/lama_3.2_1B/US_election_topics/llama_Social_Justice_and_Civil_Rights_modified.csv",
#     model_name="meta-llama/Llama-3.2-1B-Instruct",
#     hf_token="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"
# )





import os
import pandas as pd
import torch
import random
from transformers import AutoModelForCausalLM, AutoTokenizer

def generate_preference_responses(file_path, model_name, hf_token):
    """
    Loads a dataset, compares AI-generated articles with shuffled numbers, and assigns preferences based on AI evaluation.

    Parameters:
    file_path (str): The path to the input CSV file.
    model_name (str): The Hugging Face model name (e.g., "meta-llama/Llama-3.2-1B-Instruct").
    hf_token (str): The Hugging Face API token for model access.

    Returns:
    str: Path to the updated CSV file.
    """

    # Load the dataset
    df = pd.read_csv(file_path)

    # Load LLaMA model and tokenizer
    model = AutoModelForCausalLM.from_pretrained(
        model_name,
        torch_dtype="auto",
        device_map="cuda:5",
        cache_dir="/mounts/data/proj/molly/cache",
        token=hf_token,
    )
    tokenizer = AutoTokenizer.from_pretrained(model_name, token=hf_token)

    # Define valid number sets for each preference column
    valid_number_sets = {
        1: ["1", "2"],
        2: ["1", "2", "3"],
        3: ["1", "2"],
        4: ["1", "2", "3"],
        5: ["4", "5"],
        6: ["3", "4", "5"],
        7: ["4", "5"],
        8: ["3", "4", "5"],
    }

    # Define preference prompts
    preference_prompts = {
        1: "You will read two full-length articles. Based on clarity, factual accuracy, and coherence, strictly respond with '1' or '2'.\n\n The best response is:",
        2: "You will read three full-length articles. Based on clarity, factual accuracy, and coherence, strictly respond with '1', '2', or '3'.\n\n The best response is:",
        3: "Which of these two responses is less biased? Strictly respond with '1' or '2'.\n\n The best response is:",
        4: "Which of these three responses is less biased? Strictly respond with '1', '2', or '3'.\n\n The best response is:",
        5: "You will read two full-length articles. Based on clarity, factual accuracy, and coherence, strictly respond with '4' or '5'.\n\n The best response is:",
        6: "You will read three full-length articles. Based on clarity, factual accuracy, and coherence, strictly respond with '3', '4', or '5'.\n\n The best response is:",
        7: "Which of these two responses is less biased? Strictly respond with '4' or '5'.\n\n The best response is:",
        8: "Which of these three responses is less biased? Strictly respond with '3', '4', or '5'.\n\n The best response is:",
    }

    # Define prompt response mappings
    prompt_columns = ["prompt_combo_1", "prompt_combo_2", "prompt_combo_3", "prompt_combo_4", "prompt_combo_5"]

    # Generate preference columns
    for i in range(1, 9):
        pref_col = f"preference_combo_{i}"
        df[pref_col] = None  # Initialize column

        # Determine which responses to compare
        if i in [1, 3]:  
            choices = ["prompt_combo_1", "prompt_combo_2"]
        elif i in [2, 4]:  
            choices = ["prompt_combo_1", "prompt_combo_2", "prompt_combo_3"]
        elif i in [5, 7]:  
            choices = ["prompt_combo_4", "prompt_combo_5"]
        elif i in [6, 8]:  
            choices = ["prompt_combo_3", "prompt_combo_4", "prompt_combo_5"]

        # Process each row using AI model
        for idx, row in df.iterrows():
            # Create mapping of numbers to actual response texts
            original_mapping = {choice: row[choice] for choice in choices if pd.notna(row[choice])}

            # Shuffle the numbers and create a mapping
            shuffled_numbers = random.sample(valid_number_sets[i], len(original_mapping))
            shuffled_mapping = dict(zip(shuffled_numbers, original_mapping.values()))

            # Construct chat messages with shuffled response numbers
            messages = [
                {"role": "system", "content": "You are a helpful AI assistant"},
                {"role": "user", "content": preference_prompts[i] + "\n\n" + "\n\n".join([f"{num}: {text}" for num, text in shuffled_mapping.items()])}
            ]

            def generate_response():
                text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
                model_inputs = tokenizer([text], return_tensors="pt").to(model.device)
                generated_ids = model.generate(
                    **model_inputs,
                    max_new_tokens=10,  
                    do_sample=False  # üî• Greedy decoding - always chooses the most probable token
                )
                return tokenizer.decode(generated_ids[0], skip_special_tokens=True).strip()

            # Generate the AI response using greedy decoding
            response = generate_response()
            extracted_preference = next((word for word in response.split() if word in valid_number_sets[i]), None)

            # If AI fails to pick a valid number, force a retry with stricter constraints
            if extracted_preference is None:
                print(f"‚ö†Ô∏è AI failed to provide a valid response for row {idx}. Retrying with a stricter prompt...")
                messages[0]["content"] = "You MUST select a response. Choose from " + ", ".join(valid_number_sets[i]) + " only."
                response = generate_response()
                extracted_preference = next((word for word in response.split() if word in valid_number_sets[i]), None)

            # If AI still fails, default to the lowest valid choice
            if extracted_preference is None:
                extracted_preference = min(valid_number_sets[i])  # Pick the lowest valid number
                print(f"‚ö†Ô∏è AI failed twice. Assigning default selection '{extracted_preference}'.")

            # Map back to the original response
            df.at[idx, pref_col] = [key for key, value in shuffled_mapping.items() if value == shuffled_mapping[extracted_preference]][0]

    # Generate output file path
    output_file_path = file_path.replace(".csv", "_updated_final_greedy.csv")
    df.to_csv(output_file_path, index=False)

    print(f"‚úÖ Updated dataset saved as: {output_file_path}")
    return output_file_path  # Return the updated file path

def process_all_files_in_directory(directory_path, model_name, hf_token):
    """
    Iterates over all CSV files in a directory and applies the preference generation function.
    
    Parameters:
    directory_path (str): Path to the folder containing CSV files.
    model_name (str): The Hugging Face model name.
    hf_token (str): The Hugging Face API token.
    
    Returns:
    None
    """
    for filename in os.listdir(directory_path):
        if filename.endswith(".csv"):
            file_path = os.path.join(directory_path, filename)
            print(f"Processing: {file_path}")
            updated_file = generate_preference_responses(file_path, model_name, hf_token)
            print(f"‚úÖ Finished processing {file_path}. Updated file saved as: {updated_file}")

# Example usage
process_all_files_in_directory(
    directory_path="/mounts/data/proj/molly/LLM_bias_analysis/data/lama_3.2_1B/US_election_topics",
    model_name="meta-llama/Llama-3.2-1B-Instruct",
    hf_token="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"
)